{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file\n",
    "with open(\"data/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "# Used to map carrier names to the ones BigSchedule uses and supports\n",
    "with open(\"data/carrier_mapping.json\", \"r\") as f:\n",
    "    carrier_mapping = json.load(f)\n",
    "\n",
    "# Bigschedule login\n",
    "with open(\"data/bigschedule_login.json\", \"r\") as f:\n",
    "    bs_login = json.load(f)\n",
    "    \n",
    "# Prepare base information\n",
    "# UNLOCODE to port name mapping\n",
    "port_mapping = (\n",
    "    pd.concat([pd.read_csv(p, usecols=[1, 2, 4, 5], engine='python', names=[\n",
    "              'country', 'port', 'name', 'subdiv']) for p in Path('data').glob(\"*UNLOCODE CodeListPart*\")])\n",
    "    .query('port == port')\n",
    "    .assign(\n",
    "        uncode=lambda x: x.country.str.cat(x.port),\n",
    "        full_name=lambda x: np.where(\n",
    "            x.subdiv.notnull(), x.name.str.cat(x.subdiv, sep=\", \"), x.name)\n",
    "    )\n",
    "    .drop_duplicates('uncode')\n",
    "    .set_index('uncode')\n",
    "    .to_dict('index')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vessel delay tracking file\n",
    "xl = pd.ExcelFile('Vessel Delay Tracking.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigschedules_sheet = (\n",
    "    xl.parse(pd.to_datetime(xl.sheet_names,\n",
    "                            errors='coerce',\n",
    "                            format='%d.%m.%Y').max().date().strftime('%d.%m.%Y'),\n",
    "                            parse_dates=True)\n",
    "                            .query(f\"`Fwd Agent` in {[k for k,v in carrier_mapping.items() if v != '']}\")\n",
    "                            .replace({'Fwd Agent': carrier_mapping})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get port name\n",
    "bigschedules_sheet = bigschedules_sheet.assign(\n",
    "    pol_name=lambda x: x['Port of Loading'].apply(\n",
    "        lambda y: port_mapping.get(y)['name']),\n",
    "    pod_name=lambda x: x['Port of discharge'].apply(\n",
    "        lambda y: port_mapping.get(y)['name']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what searches need to be made (splitting of concerns amongst BigSchedules, MSC & G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the searches on the BigSchedules portal\n",
    "'''\n",
    "Takes in a list of dataframe of vessels, their carriers, POL, POD.\n",
    "Shrinks the above dataframe to vessels & their carriers. Uses this new dataframe for querying BigSchedules.\n",
    "Outputs an updated dataframe of vessels & their carriers with 2 additional columns updated_eta and updated_etd.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSCExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the searches on the MSC portal\n",
    "msc_delay_sheet = (xl.parse(pd.to_datetime(xl.sheet_names,\n",
    "                            errors='coerce',\n",
    "                            format='%d.%m.%Y').max().date().strftime('%d.%m.%Y'),\n",
    "                            parse_dates=True)\n",
    "                            .query(f\"`Fwd Agent` in {['MSC']}\")\n",
    "                            .replace({'Fwd Agent': carrier_mapping})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_delay_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G2Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2Extractor:\n",
    "    def __init__(self, xl, carrier_mapping):\n",
    "        self.schedule = pd.read_excel(\"G2 Schedule New.xlsx\", skiprows=9, index_col='Unnamed: 0')\n",
    "        self.delay_sheet = (xl.parse(pd.to_datetime(xl.sheet_names,\n",
    "                                errors='coerce',\n",
    "                                format='%d.%m.%Y').max().date().strftime('%d.%m.%Y'),\n",
    "                                parse_dates=True)\n",
    "                                .query(f\"`Fwd Agent` in {['G2OCEAN']}\")\n",
    "                                .replace({'Fwd Agent': carrier_mapping}))\n",
    "        self.g2_port_map = {\n",
    "            'AUPTJ': 'Portland',\n",
    "            'AUNTL': 'Newcastle',\n",
    "            'AUGLT': 'Gladstone',\n",
    "            'NZTWI': 'Bluff',\n",
    "            'TWKHH': 'Kaohsiung',\n",
    "            'KRINC': 'Inchon',\n",
    "            'KRPUS': 'Busan',\n",
    "            'JPYOK': 'Yokohama',\n",
    "            'JPNGO': 'Nagoya',\n",
    "            'JPOSA': 'Osaka',\n",
    "            'JPTOY': 'Toyama',\n",
    "            'JPIHA': 'Niihama',\n",
    "            'HKHKG': 'Hong Kong',\n",
    "            'CNSHA': 'Shanghai'\n",
    "        }\n",
    "        \n",
    "    def get_updated_etd(self, row):\n",
    "        try:\n",
    "            # column_index_etd is the column number that points to the ETD\n",
    "            column_index_etd = np.argwhere(self.schedule.columns.str.contains(row['Vessel']))[0][0] + 1\n",
    "        except IndexError:\n",
    "            return np.nan\n",
    "        return self.schedule.loc[self.schedule.index == self.g2_port_map.get(row['Port of Loading'])].iloc[:, column_index_etd][0] \n",
    "    \n",
    "    def get_updated_eta(self, row):\n",
    "        try:\n",
    "            # column_index_eta is the column number that points to the ETA\n",
    "            column_index_eta = np.argwhere(self.schedule.columns.str.contains(row['Vessel']))[0][0]\n",
    "        except IndexError:\n",
    "            return np.nan\n",
    "        return self.schedule.loc[self.schedule.index == self.g2_port_map.get(row['Port of discharge'])].iloc[:, column_index_eta][0]\n",
    "    \n",
    "    def extract(self):\n",
    "        self.delay_sheet['updated_etd'] = self.delay_sheet.apply(self.get_updated_etd, axis=1)\n",
    "        self.delay_sheet['updated_eta'] = self.delay_sheet.apply(self.get_updated_eta, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelayReport:\n",
    "    def __init__(self):\n",
    "        # Read configuration file\n",
    "        with open(\"data/config.json\", \"r\") as f:\n",
    "            self.config = json.load(f)\n",
    "            \n",
    "        # Used to map carrier names to the ones BigSchedules uses and supports\n",
    "        with open(\"data/carrier_mapping.json\", \"r\") as f:\n",
    "            self.carrier_mapping = json.load(f)\n",
    "        \n",
    "        # BigSchedules login\n",
    "        with open(\"data/bigschedules_login.json\", \"r\") as f:\n",
    "            self.bs_login = json.load(f)\n",
    "        \n",
    "        # Prepare base information\n",
    "        # UNLOCODE to port name mapping\n",
    "        self.port_mapping = (\n",
    "            pd.concat([pd.read_csv(p, usecols=[1, 2, 4, 5], engine='python', names=[\n",
    "                      'country', 'port', 'name', 'subdiv']) for p in Path('data').glob(\"*UNLOCODE CodeListPart*\")])\n",
    "            .query('port == port')\n",
    "            .assign(uncode=lambda x: x.country.str.cat(x.port),\n",
    "                    full_name=lambda x: np.where(x.subdiv.notnull(), x.name.str.cat(x.subdiv, sep=\", \"), x.name))\n",
    "            .drop_duplicates('uncode')\n",
    "            .set_index('uncode')\n",
    "            .to_dict('index'))\n",
    "        \n",
    "        # Read the vessel delay tracking file\n",
    "        self.xl = pd.ExcelFile('Vessel Delay Tracking.xlsx')\n",
    "        # today_date = datetime.now().strftime('%d.%m.%Y')\n",
    "        # if today_date not in self.xl.sheet_names:\n",
    "        #     raise Exception(\n",
    "        #         f\"The script cannot find today's date ({today_date}) in the Vessel Delay Tracking.xlsx file provided. Please check that the sheets are correctly named - the script will only operate on a sheet with today's date.\")\n",
    "        \n",
    "    def run_bs(self):\n",
    "        if self.config.get('run_bs'):\n",
    "            bs_extractor = BSExtractor()\n",
    "            bs_extractor.extract()\n",
    "        \n",
    "    def run_msc(self):\n",
    "        if self.config.get('run_msc'):\n",
    "            self.msc_extractor = MSCExtractor(self.xl, self.carrier_mapping)\n",
    "            self.msc_extractor.extract()\n",
    "    \n",
    "    def run_g2(self):\n",
    "        if self.config.get('run_g2'):\n",
    "            self.g2_extractor = G2Extractor(self.xl, self.carrier_mapping)\n",
    "            self.g2_extractor.extract()\n",
    "    \n",
    "    def assemble(self):\n",
    "        # Assemble the final dataframe to update\n",
    "        main_delay_sheet = self.xl.parse()\n",
    "\n",
    "        # Add new columns to the right side of the dataframe\n",
    "        new_columns = ['updated_etd', 'updated_eta', 'No. of days delayed ETD', 'No. of days delayed ETA', 'Reason of Delay']\n",
    "        main_delay_sheet[new_columns] = pd.DataFrame([[pd.NaT for i in range(4)] + [np.nan]])\n",
    "\n",
    "        if self.config.get('run_bs'):\n",
    "            main_delay_sheet.update(self.bs_extractor.delay_sheet)\n",
    "        \n",
    "        if self.config.get('run_msc'):\n",
    "            main_delay_sheet.update(self.msc_extractor.delay_sheet)\n",
    "        \n",
    "        if self.config.get('run_g2'):\n",
    "            main_delay_sheet.update(self.g2_extractor.delay_sheet)\n",
    "\n",
    "        # Calculate the deltas\n",
    "        main_delay_sheet['No. of days delayed ETD'] = (main_delay_sheet.updated_etd\n",
    "                                                       - pd.to_datetime(main_delay_sheet['ETD Date'])).dt.days\n",
    "        main_delay_sheet['No. of days delayed ETA'] = (main_delay_sheet.updated_eta\n",
    "                                                       - pd.to_datetime(main_delay_sheet['Disport ETA'])).dt.days\n",
    "\n",
    "        # Format the dates correctly via strftime\n",
    "        date_columns = ['ETD Date', 'Disport ETA', 'updated_etd', 'updated_eta']\n",
    "        for column in date_columns:\n",
    "            main_delay_sheet[column] = main_delay_sheet[column].dt.strftime('%d/%m/%Y')\n",
    "        self.main_delay_sheet = main_delay_sheet.copy()\n",
    "    \n",
    "    def output(self):\n",
    "        # Output the excel file\n",
    "        saved_file = 'main_delay_sheet.xlsx'\n",
    "        main_delay_sheet.to_excel(saved_file)\n",
    "        # os.startfile(saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delay report skeleton\n",
    "delay_report = DelayReport()\n",
    "delay_report.run_bs()\n",
    "delay_report.run_msc()\n",
    "delay_report.run_g2()\n",
    "delay_report.assemble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
