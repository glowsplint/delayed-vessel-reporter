{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delay Report\n",
    "### Overview\n",
    "The delay report script aims to find the updated_eta and updated_etd of certain vessels provided within \"Vessel Delay Tracking.XLSX\". This is done by querying an underlying BigSchedules API, MSC Web API and from a static G2 Schedules Excel document. None of the API interactions use the Rio Tinto credentials to ensure that traceback cannot occur.\n",
    "\n",
    "The script is written in a modular approach to increase ease of maintenance and improve code quality. Configurations are stored in a `data` subdirectory. The script expects a `Vessel Delay Tracking.XLSX` file and `g2_filename` (G2 Schedule Excel file) in the same directory.\n",
    "\n",
    "### Features\n",
    "1. Avoids detection\n",
    "    - Uses API calls instead of Selenium which is easily detectable\n",
    "    - Uses randomised timing for API requests\n",
    "2. Modular\n",
    "    - If one component breaks, you can always disable it without affecting the other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_report import MSCExtractor, G2Extractor, DelayReport, write_json, read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file\n",
    "with open(\"data/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "# Used to map carrier names to the ones BigSchedule uses and supports\n",
    "with open(\"data/carrier_mapping.json\", \"r\") as f:\n",
    "    carrier_mapping = json.load(f)\n",
    "\n",
    "# Bigschedule login\n",
    "with open(\"data/bigschedules_login.json\", \"r\") as f:\n",
    "    bs_login = json.load(f)\n",
    "    \n",
    "# Prepare base information\n",
    "# UNLOCODE to port name mapping\n",
    "port_mapping = (\n",
    "    pd.concat([pd.read_csv(p, usecols=[1, 2, 4, 5], engine='python', names=[\n",
    "              'country', 'port', 'name', 'subdiv']) for p in Path('data').glob(\"*UNLOCODE CodeListPart*\")])\n",
    "    .query('port == port')\n",
    "    .assign(\n",
    "        uncode=lambda x: x.country.str.cat(x.port),\n",
    "        full_name=lambda x: np.where(\n",
    "            x.subdiv.notnull(), x.name.str.cat(x.subdiv, sep=\", \"), x.name)\n",
    "    )\n",
    "    .drop_duplicates('uncode')\n",
    "    .set_index('uncode')\n",
    "    .to_dict('index')\n",
    ")\n",
    "\n",
    "# Read the vessel delay tracking file\n",
    "xl = pd.ExcelFile('Vessel Delay Tracking.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigschedules_sheet = (\n",
    "    xl.parse(pd.to_datetime(xl.sheet_names,\n",
    "                            errors='coerce',\n",
    "                            format='%d.%m.%Y').max().date().strftime('%d.%m.%Y'),\n",
    "                            parse_dates=True)\n",
    "                            .query(f\"`Fwd Agent` in {[k for k,v in carrier_mapping.items()]}\")\n",
    "                            .replace({'Fwd Agent': carrier_mapping})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get port name\n",
    "bigschedules_sheet = bigschedules_sheet.assign(pol_name=lambda x: x['Port of Loading'].apply(lambda y: port_mapping.get(y)['name']),\n",
    "                                               pod_name=lambda x: x['Port of discharge'].apply(lambda y: port_mapping.get(y)['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigschedules_sheet.loc[bigschedules_sheet['Fwd Agent'] == 'HAMBURG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSExtractor:\n",
    "    \"\"\"\n",
    "    Extracts information from the BigSchedules Sailing Schedules Web API (not Vessel Tracking Web API).\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    prepare:\n",
    "        A single query to the BigSchedules Web API can provide information to multiple lines on the delay_sheet.\n",
    "        Further filters self.delay_sheet to a smaller list of searches needed to fulfill all the lines on the\n",
    "            delay_sheet. This reduces the total number of calls made to the BigSchedules Web API and prevents\n",
    "            duplication of API calls.\n",
    "        \n",
    "    call_api:\n",
    "        Makes calls to the BigSchedules Web API, using information from the prepare method as parameters in the\n",
    "        API request. Also saves the API responses into a subdirectory \"responses/<today_date>\".\n",
    "    \n",
    "    extract:\n",
    "        Extracts information from the JSON responses from the call_api method and assembles the final dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(self, main_delay_sheet: pd.DataFrame, interval: tuple):\n",
    "        # Get the BigSchedules delay sheet\n",
    "        self.delay_sheet = (main_delay_sheet.query(f\"`Fwd Agent` not in {['MSC', 'G2OCEAN']}\")\n",
    "                            .drop(['updated_etd', 'updated_eta', 'No. of days delayed ETD',\n",
    "                                   'No. of days delayed ETA', 'Reason of Delay'], axis=1)\n",
    "                            .copy())\n",
    "\n",
    "        # Get the BigSchedules-specific port names from the UNLOCODEs\n",
    "        self.port_mapping = (pd.concat([pd.read_csv(p, usecols=[1, 2, 4, 5], engine='python',\n",
    "                                               names=['country', 'port', 'name', 'subdiv']) for p in Path('data').glob(\"*UNLOCODE CodeListPart*\")])\n",
    "            .query('port == port')\n",
    "            .assign(\n",
    "                uncode=lambda x: x.country.str.cat(x.port),\n",
    "                full_name=lambda x: np.where(\n",
    "                    x.subdiv.notnull(), x.name.str.cat(x.subdiv, sep=\", \"), x.name)\n",
    "            )\n",
    "            .drop_duplicates('uncode')\n",
    "            .set_index('uncode')\n",
    "            .to_dict('index')\n",
    "        )\n",
    "        \n",
    "        # Get port name\n",
    "        self.delay_sheet = self.delay_sheet.assign(pol_name=lambda x: x['Port of Loading'].apply(lambda y: self.port_mapping.get(y)),\n",
    "                                                   pod_name=lambda x: x['Port of discharge'].apply(lambda y: self.port_mapping.get(y))).copy()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def prepare(self):\n",
    "        \"\"\"\n",
    "        Further filters self.delay_sheet to a smaller list of searches needed to fulfill all the lines on the\n",
    "            delay_sheet.\n",
    "        \"\"\"\n",
    "#         # Further filter by POL-Vessel-Voyage to get ETD, POD-Vessel-Voyage to get ETA\n",
    "#         key = ['pol_name', 'pod_name']\n",
    "#         self.reduced_df = self.delay_sheet.drop_duplicates(key)[key].sort_values(key)\n",
    "\n",
    "#         self.reduced_df['pol_code'] = self.reduced_df.pol_name.map(self.msc_port_id)\n",
    "#         self.reduced_df['pod_code'] = self.reduced_df.pod_name.map(self.msc_port_id)\n",
    "\n",
    "#         # Unable to handle those with no pod_id in BigSchedules Web; dropping these lines\n",
    "#         self.reduced_df.dropna(inplace=True)\n",
    "        \n",
    "    def call_api(self):\n",
    "        \"\"\"\n",
    "        Makes calls to the BigSchedules Web API, using information from the prepare method as parameters in the\n",
    "        API request. Also saves the API responses into a subdirectory \"responses/<today_date>\".\n",
    "        \"\"\"\n",
    "#         def get_schedules(etd: str, pol: str, pod: str):\n",
    "#             url = f\"https://www.bigschedules.com//api/vesselSchedule/list?DISABLE_ART=true&_=2020081917&carrierId=18&language=en-US&scac=HLCU&vesselGid=V000005557&vesselName=CHRISTA+SCHULTE\"\n",
    "#             headers = {\n",
    "#                 'Accept': 'application/json',\n",
    "#                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36',\n",
    "#                 'Content-Type': 'application/json',\n",
    "#                 'Sec-Fetch-Site': 'same-origin',\n",
    "#                 'Sec-Fetch-Mode': 'cors',\n",
    "#                 'Sec-Fetch-Dest': 'empty',\n",
    "#                 'Referer': 'https://www.msc.com/search-schedules',\n",
    "#                 'Accept-Language': 'en-GB,en;q=0.9',\n",
    "#                 'Cookie': 'CMSPreferredCulture=en-GB; ASP.NET_SessionId=tht5lkut0asln2goiskoagfe; UrlReferrer=https://www.google.com/; CurrentContact=8b0b2fea-705b-4a4f-b8bf-bb1cd6c982bc; MSCAgencyId=115867; BIGipServerkentico.app~kentico_pool=439883018.20480.0000; _ga=GA1.2.1736073830.1597290148; _gid=GA1.2.1289141279.1597290148; _gcl_au=1.1.345060449.1597290148; __hstc=100935006.13bb76c8a78a8d0a203a993ffef3a3f6.1597290148282.1597290148282.1597290148282.1; hubspotutk=13bb76c8a78a8d0a203a993ffef3a3f6; __hssrc=1; _ym_uid=15972901491036911544; _ym_d=1597290149; _ym_isad=1; newsletter-signup-cookie=temp-hidden; _hjid=3e183004-f562-4048-8b60-daccdf9c187c; _hjUserAttributesHash=2c3b62a0e1cd48bdfd4d01b922060e19; _hjCachedUserAttributes={\"attributes\":{\"mscAgencyId\":\"115867\"},\"userId\":null}; OptanonAlertBoxClosed=2020-08-13T03:42:45.080Z; CMSCookieLevel=200; VisitorStatus=11062214903; TS0142aef9=0192b4b6225179b1baa3b4d270b71a4eee782a0192338173beabaa471f306c2a13fe854bf6a7ac08ac21924991864aa7728c54559023beabd273d82285d5f943202adb58da417d61813232e89b240828c090f890c6a74dc4adfec38513d13447be4b5b4404d69f964987b7917f731b858f0c9880a139994b98397c4aeb5bd60b0d0e38ec9e5f3c97b13fb184b4e068506e6086954f8a515f2b7239d2e5c1b9c70f61ca74f736355c58648a6036e9b5d06412389ac41221c5cb740df99c84dc2bfef4a530dbc5e2577c189212eebac723d9ee9f98030f4bc6ca7d824ab313ae5fdd1eaa9886; OptanonConsent=isIABGlobal=false&datestamp=Thu+Aug+13+2020+11%3A43%3A36+GMT%2B0800+(Singapore+Standard+Time)&version=5.9.0&landingPath=NotLandingPage&groups=1%3A1%2C2%3A1%2C3%3A1%2C4%3A1%2C0_53017%3A1%2C0_53020%3A1%2C0_53018%3A1%2C0_53019%3A1%2C101%3A1&AwaitingReconsent=false'\n",
    "#             }\n",
    "#             response = self.session.get(url, headers=headers)\n",
    "#             return response\n",
    "        \n",
    "#         self.response_jsons = []\n",
    "#         first_day = datetime.today().replace(day=1).strftime('%Y-%m-%d')\n",
    "        \n",
    "#         for row in tqdm(self.reduced_df.itertuples(), total=len(self.reduced_df)):\n",
    "#             response_filename = f'MSC {int(row.pol_code)}-{int(row.pod_code)}.json'\n",
    "#             if response_filename not in os.listdir():\n",
    "#                 response = get_schedules(first_day, int(row.pol_code), int(row.pod_code))\n",
    "#                 self.response_jsons.append(response.json())\n",
    "#                 write_json(response.json(), response_filename)\n",
    "#                 time.sleep(random.randint(*self.interval))\n",
    "#             else:\n",
    "#                 with open(response_filename, 'r') as f:\n",
    "#                     self.response_jsons.append(json.load(f))\n",
    "        \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extracts information from the JSON responses from the call_api method and assembles the final dataframe.\n",
    "        \"\"\"\n",
    "#         def get_relevant_fields(response, i):\n",
    "#             return {\n",
    "#                 'pol_code': response[0]['Sailings'][i]['PortOfLoadId'],\n",
    "#                 'pod_code': response[0]['Sailings'][i]['PortOfDischargeId'],\n",
    "#                 'Voyage': response[0]['Sailings'][i]['VoyageNum'],\n",
    "#                 'Vessel': response[0]['Sailings'][i]['VesselName'],\n",
    "#                 'updated_etd': response[0]['Sailings'][i]['NextETD'],\n",
    "#                 'updated_eta': response[0]['Sailings'][i]['ArrivalDate']\n",
    "#             }\n",
    "\n",
    "#         self.response_df = pd.DataFrame(([get_relevant_fields(response, i)\n",
    "#                                      for response in self.response_jsons\n",
    "#                                      for i in range(len(response[0]['Sailings']))\n",
    "#                                      if len(response)\n",
    "#                                     ]))\n",
    "        \n",
    "#         # Create reverse mapping from port_code to name\n",
    "#         msc_port_id_reversed = {v:k for k,v in self.msc_port_id.items()}\n",
    "\n",
    "#         # Add additional columns to response_df\n",
    "#         self.response_df['pol_name'] = self.response_df.pol_code.map(msc_port_id_reversed)\n",
    "#         self.response_df['pod_name'] = self.response_df.pod_code.map(msc_port_id_reversed)\n",
    "\n",
    "#         # Merge results back to original dataframe\n",
    "#         merge_key = ['pol_name', 'pod_name', 'Vessel', 'Voyage']\n",
    "#         self.delay_sheet = (self.delay_sheet.reset_index().\n",
    "#                             merge(self.response_df[merge_key + ['updated_eta', 'updated_etd']],\n",
    "#                                   on=merge_key, how='left')\n",
    "#                             .set_index('index')\n",
    "#                             .copy())\n",
    "#         self.delay_sheet.updated_eta = pd.to_datetime(self.delay_sheet.updated_eta.str[:10])\n",
    "#         self.delay_sheet.updated_etd = pd.to_datetime(self.delay_sheet.updated_etd.str[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do\n",
    "1. I need to figure out how to get the first cookie and use it in subsequent headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')\n",
    "# Delay report skeleton\n",
    "delay_report = DelayReport()\n",
    "delay_report.run_bs()\n",
    "delay_report.run_msc()\n",
    "delay_report.run_g2()\n",
    "delay_report.calculate_deltas()\n",
    "delay_report.output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
